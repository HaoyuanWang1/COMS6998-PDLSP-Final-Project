{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "COMS6998 Final Project",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.util import dispatch\n",
        "from typing import Union\n",
        "import random\n",
        "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
        "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
        "\n",
        "\n",
        "class AbftTensor(tf.experimental.BatchableExtensionType):\n",
        "  # __name__ is required for serialization in SavedModel; see below for details.\n",
        "  __name__ = 'extension_type_colab.AbftTensor'\n",
        "\n",
        "  values: tf.Tensor\n",
        "\n",
        "  shape = property(lambda self: self.values.shape)\n",
        "  def set_shape(self, value):\n",
        "        print(f\"Setting shape {value} from {self.values.shape}\")\n",
        "        self.values.set_shape = value\n",
        "\n",
        "  dtype = property(lambda self: self.values.dtype)\n",
        "\n",
        "  def __init__(self, values):\n",
        "      self.values = values\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.values)\n",
        "\n",
        "  def row_checksum(self):\n",
        "    # print(f\"Row: {self.row_checksum.shape}\")\n",
        "    return tf.math.reduce_sum(self.values, 0, keepdims=1)\n",
        "\n",
        "  def col_checksum(self):\n",
        "    # print(f\"Col: {self.col_checksum.shape}\")\n",
        "    return tf.math.reduce_sum(self.values, 1, keepdims=1)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    return AbftTensor(self.values[item])\n",
        "    [item]\n",
        "  # def __validate__(self):\n",
        "  #   assert tf.math.reduce_all(self.checksum == tf.math.reduce_sum(self.values, range(1, len(self.values.shape)))), 'Checksum failed!'\n",
        "\n",
        "  # def with_default(self, default):\n",
        "  #   return tf.where(self.checksum, self.values, default)\n",
        "\n",
        "  # def __repr__(self):\n",
        "  #   return abft_tensor_str(self.values, self.checksum)\n",
        "\n",
        "  class Spec:\n",
        "    def __init__(self, shape, dtype=tf.float32):\n",
        "      self.values = tf.TensorSpec(shape, dtype)\n",
        "      # self.row_checksum = tf.TensorSpec((1,shape[1],), dtype)\n",
        "      # self.col_checksum = tf.TensorSpec((shape[0],1), dtype)\n",
        "\n",
        "    shape = property(lambda self: self.values.shape)\n",
        "    dtype = property(lambda self: self.values.dtype)\n",
        "\n",
        "\n",
        "\n",
        "def convert_to_abft_tensor(x):\n",
        "  if isinstance(x, AbftTensor):\n",
        "    return x\n",
        "  else:\n",
        "    return AbftTensor(x)\n",
        "\n",
        "\n",
        "\n",
        "@tf.experimental.dispatch_for_unary_elementwise_apis(AbftTensor)\n",
        "def unary_elementwise_op_handler(op, x):\n",
        " return AbftTensor(op(x.values))\n",
        "\n",
        "@tf.experimental.dispatch_for_binary_elementwise_apis(\n",
        "    Union[AbftTensor, tf.Tensor],\n",
        "    Union[AbftTensor, tf.Tensor])\n",
        "def binary_elementwise_op_handler(op, x, y):\n",
        "  x = convert_to_abft_tensor(x)\n",
        "  y = convert_to_abft_tensor(y)\n",
        "  return AbftTensor(op(x.values, y.values))\n",
        "\n",
        "from TensorFI2.src import tensorfi2 as tfi\n",
        "\n",
        "\n",
        "abft_inject_faults: bool = False\n",
        "abft_detect_faults: bool = False\n",
        "abft_throw_faults: bool = False\n",
        "\n",
        "\n",
        "@tf.experimental.dispatch_for_api(tf.matmul)\n",
        "def abft_matmul(a: AbftTensor, b,\n",
        "                  transpose_a=False, transpose_b=False,\n",
        "                  adjoint_a=False, adjoint_b=False,\n",
        "                  a_is_sparse=False, b_is_sparse=False,\n",
        "                  output_type=None):\n",
        "  # print(f\"mult  a:{a.shape} b:{b.shape}\")\n",
        "  if isinstance(b, AbftTensor):\n",
        "    b_abft = b\n",
        "    b = b.values\n",
        "  c = AbftTensor(tf.matmul(a.values, b, transpose_a, transpose_b, adjoint_a,\n",
        "                  adjoint_b, a_is_sparse, b_is_sparse, output_type))\n",
        "  if tf.executing_eagerly() and abft_inject_faults:\n",
        "    # print(\"Injecting!\")\n",
        "    flip_r = random.randint(0, c.shape[0]-1)\n",
        "    flip_c = random.randint(0, c.shape[1]-1)\n",
        "\n",
        "    pos = random.randint(0, 31)\n",
        "    indices = [[flip_r, flip_c]]  \n",
        "    values = [tfi.bitflip(c.values[flip_r][flip_c], pos)]\n",
        "    delta = tf.SparseTensor(indices, values, c.shape)\n",
        "    result = c.values + tf.sparse.to_dense(delta)\n",
        "    c = AbftTensor(result)\n",
        "\n",
        "  # print(f\"Checksum S_a{a.row_checksum().shape} B: {b.shape}\")\n",
        "  if abft_detect_faults:\n",
        "    s_ab = tf.matmul(a.row_checksum(), b)\n",
        "    # ab_s = tf.matmul(a.values, b.checksum)\n",
        "    c_rows_sum = tf.reduce_sum(c.values,0, keepdims=1)\n",
        "    # print(f\"s_ab: {s_ab}\")\n",
        "    # print(f\"c_rows_sum: {c_rows_sum}\")\n",
        "    if tf.executing_eagerly():\n",
        "      no_fault_detected = tf.experimental.numpy.allclose(s_ab,c_rows_sum, atol=1e-04)\n",
        "      if abft_throw_faults:\n",
        "        assert no_fault_detected, f\"Abft failure! {s_ab} != {c_rows_sum}\"\n",
        "    # else:\n",
        "    #   print(\"No checksum since delayed.\")\n",
        "  return c\n",
        "  \n",
        "@tf.experimental.dispatch_for_api(tf.tensordot)\n",
        "def abft_tensordot(a: AbftTensor, b, axes, name=None):\n",
        "  # print(f\"dot {a.shape}{b.shape}\")\n",
        "  if isinstance(a, AbftTensor):\n",
        "    a = a.values\n",
        "  if isinstance(a, AbftTensor):\n",
        "    b = b.values\n",
        "  return AbftTensor(tf.tensordot(a, b, axes, name))\n",
        "\n",
        "@tf.experimental.dispatch_for_api(tf.convert_to_tensor)\n",
        "def abft_convert_to_tensor(value: AbftTensor, dtype=None, dtype_hint=None, name=None):\n",
        "  # print(\"Convert\")\n",
        "  if isinstance(value, AbftTensor):\n",
        "    return tf.convert_to_tensor(value.values, dtype, dtype_hint, name)\n",
        "  else:\n",
        "    return tf.convert_to_tensor(value, dtype, dtype_hint, name)\n",
        "\n",
        "\n",
        "from tensorflow.python.framework import ops\n",
        "\n",
        "from tensorflow.python.framework import tensor_conversion_registry\n",
        "\n",
        "def conversion_func(value, dtype=None, name=None, as_ref=False):\n",
        "  return ops.convert_to_tensor(value.values, dtype, name,as_ref)\n",
        "\n",
        "tf.register_tensor_conversion_function(\n",
        "    AbftTensor, conversion_func, priority=100\n",
        ")\n"
      ],
      "metadata": {
        "id": "I0SJ8k8g1o-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import keras\n",
        "import time\n",
        "\n",
        "\n",
        "def do_boston(fault_injection: bool, abft: bool, epochs: int = 10, throw_abft_error = False,verbose = True):\n",
        "  print(f\"Training and Executing Boston Fault Injection:{fault_injection} ABFT: {abft}\")\n",
        "  global abft_inject_faults\n",
        "  abft_inject_faults = fault_injection\n",
        "  global abft_detect_faults\n",
        "  abft_detect_faults = abft\n",
        "  global abft_throw_faults\n",
        "  abft_throw_faults = throw_abft_error\n",
        "\n",
        "\n",
        "  import os\n",
        "  os.environ['PYTHONHASHSEED']=str(66)\n",
        "\n",
        "  import random\n",
        "  random.seed(66)\n",
        "\n",
        "  np.random.seed(66)\n",
        "  tf.random.set_seed(66)\n",
        "\n",
        "  os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "  os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "  \n",
        "\n",
        "  (x_train, y_train), (x_test, y_test) = datasets.boston_housing.load_data()\n",
        "  train_mean = np.mean(x_train, axis=0)\n",
        "  train_std = np.std(x_train, axis=0)\n",
        "  train_features = (x_train - train_mean) / train_std\n",
        "  test_features_norm = (x_test - train_mean) / train_std\n",
        "\n",
        "\n",
        "\n",
        "  if abft or fault_injection:\n",
        "    def abft_relu(x):\n",
        "      return AbftTensor(keras.activations.relu(x))\n",
        "    class abft_glorot_uniform(keras.initializers.glorot_uniform):\n",
        "      def __call__(self, shape, dtype=None, **kwargs):\n",
        "        return AbftTensor(super(abft_glorot_uniform, self).__call__(shape, dtype, **kwargs))\n",
        "\n",
        "    input_spec = AbftTensor.Spec(train_features.shape, tf.float32)\n",
        "    x_train = AbftTensor(x_train)\n",
        "    train_features = AbftTensor(train_features)\n",
        "    test_features_norm = AbftTensor(test_features_norm)\n",
        "    relu_func = abft_relu\n",
        "    init_class = abft_glorot_uniform\n",
        "  else:\n",
        "    input_spec = tf.TensorSpec(train_features.shape, tf.float32)\n",
        "    relu_func = keras.activations.relu\n",
        "    init_class = keras.initializers.glorot_uniform\n",
        "\n",
        "\n",
        "\n",
        "  boston_model = models.Sequential()\n",
        "  boston_model.add(layers.InputLayer(type_spec=input_spec))\n",
        "  boston_model.add(layers.Dense(20,activation=relu_func, kernel_initializer=init_class(seed=66)))\n",
        "  boston_model.add(layers.Dense(1,activation=relu_func, kernel_initializer=init_class(seed=66)))\n",
        "\n",
        "  # boston_model.summary()\n",
        "\n",
        "  boston_model.compile(optimizer='adam',\n",
        "                loss='mse',\n",
        "                metrics=['mae', 'mse'], run_eagerly=True)\n",
        "\n",
        "  start=time.time()\n",
        "  early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n",
        "  history = boston_model.fit(train_features, y_train, epochs=epochs, verbose=verbose, validation_data=(test_features_norm,y_test), shuffle=False,\n",
        "                      callbacks=[early_stop])\n",
        "  end=time.time()\n",
        "  rmse_final = np.sqrt(float(history.history['val_mse'][-1]))\n",
        "  print(f\"RMSE Error: {rmse_final}\")\n",
        "  print(\"training_time:\",end-start)\n",
        "  return (end-start), rmse_final\n",
        "\n",
        "print(\"Executing test cases\")\n",
        "training_epochs = 5\n",
        "_,sdc_1 = do_boston(False, False, training_epochs)\n",
        "_,sdc_2 = do_boston(False, False, training_epochs)\n",
        "assert sdc_1 == sdc_2, \"RMS should be same due to deterministic behaviour\" \n",
        "\n",
        "_,sdc_abft = do_boston(False, True, training_epochs, False)\n",
        "assert sdc_1 == sdc_abft, f\"RMS should be same due to deterministic behaviour {sdc_1} {sdc_abft}\" \n",
        "\n",
        "_,sdc_fi = do_boston(True, False, training_epochs)\n",
        "import math\n",
        "assert sdc_1 < sdc_fi or math.isnan(sdc_fi), \"FI RMS should be more! {sdc_1} < {sdc_fi}\" \n",
        "\n",
        "try:\n",
        "  _,sdc_fi_abft = do_boston(True, True, training_epochs, True)\n",
        "  raise RuntimeError(\"Should not reach here.  ABFT should have detected a fault\")\n",
        "except AssertionError as ae:\n",
        "  print(f\"Successfully caught assertion error {ae}\")\n",
        "\n",
        "_,sdc_fi_abft = do_boston(True, True, training_epochs, False)\n",
        "assert sdc_1 < sdc_fi_abft or math.isnan(sdc_fi_abft), \"FI RMS should be more! {sdc_1} < {sdc_fi_abft}\" \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCmNL-odb_6A",
        "outputId": "39e96b18-9c79-47bb-b2b9-e4d8392ba4ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing test cases\n",
            "Training and Executing Boston Fault Injection:False ABFT: False\n",
            "Epoch 1/5\n",
            "13/13 [==============================] - 1s 26ms/step - loss: 575.3324 - mae: 22.1941 - mse: 575.3324 - val_loss: 598.6422 - val_mae: 22.7482 - val_mse: 598.6422\n",
            "Epoch 2/5\n",
            "13/13 [==============================] - 0s 20ms/step - loss: 568.4198 - mae: 22.0662 - mse: 568.4198 - val_loss: 589.6136 - val_mae: 22.5767 - val_mse: 589.6136\n",
            "Epoch 3/5\n",
            "13/13 [==============================] - 0s 21ms/step - loss: 558.8830 - mae: 21.8850 - mse: 558.8830 - val_loss: 578.3877 - val_mae: 22.3618 - val_mse: 578.3877\n",
            "Epoch 4/5\n",
            "13/13 [==============================] - 0s 22ms/step - loss: 548.1387 - mae: 21.6771 - mse: 548.1387 - val_loss: 566.4680 - val_mae: 22.1304 - val_mse: 566.4680\n",
            "Epoch 5/5\n",
            "13/13 [==============================] - 0s 23ms/step - loss: 536.7958 - mae: 21.4550 - mse: 536.7958 - val_loss: 553.9682 - val_mae: 21.8857 - val_mse: 553.9682\n",
            "RMSE Error: 23.53652907043844\n",
            "training_time: 1.950382947921753\n",
            "Training and Executing Boston Fault Injection:False ABFT: False\n",
            "Epoch 1/5\n",
            "13/13 [==============================] - 0s 21ms/step - loss: 575.3324 - mae: 22.1941 - mse: 575.3324 - val_loss: 598.6422 - val_mae: 22.7482 - val_mse: 598.6422\n",
            "Epoch 2/5\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 568.4198 - mae: 22.0662 - mse: 568.4198 - val_loss: 589.6136 - val_mae: 22.5767 - val_mse: 589.6136\n",
            "Epoch 3/5\n",
            "13/13 [==============================] - 0s 24ms/step - loss: 558.8830 - mae: 21.8850 - mse: 558.8830 - val_loss: 578.3877 - val_mae: 22.3618 - val_mse: 578.3877\n",
            "Epoch 4/5\n",
            "13/13 [==============================] - 0s 22ms/step - loss: 548.1387 - mae: 21.6771 - mse: 548.1387 - val_loss: 566.4680 - val_mae: 22.1304 - val_mse: 566.4680\n",
            "Epoch 5/5\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 536.7958 - mae: 21.4550 - mse: 536.7958 - val_loss: 553.9682 - val_mae: 21.8857 - val_mse: 553.9682\n",
            "RMSE Error: 23.53652907043844\n",
            "training_time: 1.446770429611206\n",
            "Training and Executing Boston Fault Injection:False ABFT: True\n",
            "Epoch 1/5\n",
            "13/13 [==============================] - 0s 23ms/step - loss: 575.3324 - mae: 22.1941 - mse: 575.3324 - val_loss: 598.6422 - val_mae: 22.7482 - val_mse: 598.6422\n",
            "Epoch 2/5\n",
            "13/13 [==============================] - 0s 22ms/step - loss: 568.4198 - mae: 22.0662 - mse: 568.4198 - val_loss: 589.6136 - val_mae: 22.5767 - val_mse: 589.6136\n",
            "Epoch 3/5\n",
            "13/13 [==============================] - 0s 20ms/step - loss: 558.8830 - mae: 21.8850 - mse: 558.8830 - val_loss: 578.3877 - val_mae: 22.3618 - val_mse: 578.3877\n",
            "Epoch 4/5\n",
            "13/13 [==============================] - 0s 23ms/step - loss: 548.1387 - mae: 21.6771 - mse: 548.1387 - val_loss: 566.4680 - val_mae: 22.1304 - val_mse: 566.4680\n",
            "Epoch 5/5\n",
            "13/13 [==============================] - 0s 23ms/step - loss: 536.7958 - mae: 21.4550 - mse: 536.7958 - val_loss: 553.9682 - val_mae: 21.8857 - val_mse: 553.9682\n",
            "RMSE Error: 23.53652907043844\n",
            "training_time: 1.4973795413970947\n",
            "Training and Executing Boston Fault Injection:True ABFT: False\n",
            "Epoch 1/5\n",
            "13/13 [==============================] - 0s 25ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
            "Epoch 2/5\n",
            "13/13 [==============================] - 0s 26ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
            "Epoch 3/5\n",
            "13/13 [==============================] - 0s 23ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
            "Epoch 4/5\n",
            "13/13 [==============================] - 0s 28ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
            "Epoch 5/5\n",
            "13/13 [==============================] - 0s 24ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
            "RMSE Error: nan\n",
            "training_time: 2.596184492111206\n",
            "Training and Executing Boston Fault Injection:True ABFT: True\n",
            "Epoch 1/5\n",
            "Successfully caught assertion error Exception encountered when calling layer \"dense_9\" (type Dense).\n",
            "\n",
            "Abft failure! [[-3.8125055]] != [[-5.1148524]]\n",
            "\n",
            "Call arguments received:\n",
            "  â€¢ inputs=AbftTensor(values=<tf.Tensor: shape=(32, 20), dtype=float32, numpy=\n",
            "array([[0.00000000e+00, 4.69070494e-01, 6.07342720e-01, 0.00000000e+00,\n",
            "        0.00000000e+00, 0.00000000e+00, 8.80192444e-02, 0.00000000e+00,\n",
            "        5.65022707e-01, 0.00000000e+00, 0.00000000e+00, 2.34142646e-01,\n",
            "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
            "        4.49136704e-01, 1.94808960e-01, 0.00000000e+00, 1.24870574e+00],\n",
            "       [5.64507008e-01, 0.00000000e+00, 0.00000000e+00, 1.15665877e+00,\n",
            "        1.59428203e+00, 1.76036274e+00, 0.00000000e+00, 1.29027653e+00,\n",
            "        0.00000000e+00, 1.53794885e+00, 0.00000000e+00, 7.24319696e-01,\n",
            "        1.09115982e+00, 0.00000000e+00, 5.50113976e-01, 0.00000000e+00,\n",
            "        9.40278992e-02, 0.00000000e+00, 1.45393097e+00, 0.00000000e+00],\n",
            "       [1.32366276e+00, 5.16467333e-01, 4.01621461e-01, 5.76825917e-01,\n",
            "        2.88772047e-01, 0.00000000e+00, 6.21302903e-01, 3.16670567e-01,\n",
            "        0.00000000e+00, 0.00000000e+00, 3.58590215e-01, 0.00000000e+00,\n",
            "        0.00000000e+00, 4.45995867e-01, 0.00000000e+00, 1.54904628e+00,\n",
            "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.39728266e-01,\n",
            "        1.06431328e-01, 9.80333388e-01, 7.40750968e-01, 4.45875712e-02,\n",
            "        1.64563406e+00, 5.87322749e-02, 0.00000000e+00, 0.00000000e+00,\n",
            "        0.00000000e+00, 0.00000000e+00, 6.34061456e-01, 0.00000000e+00,\n",
            "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.29868734e+00],\n",
            "       [1.00197327e+00, 4.42668915e-01, 1.03068101e+00, 0.00000000e+00,\n",
            "        8.23103450e-03, 0.00000000e+00, 0.00000000e+00, 4.23632711e-01,\n",
            "        0.00000000e+00, 0.00000000e+00, 1.35101736e+00, 3.54992598e-01,\n",
            "        2.02766731e-01, 2.21964747e-01, 0.00000000e+00, 1.39010262e+00,\n",
            "        3.36772561e-01, 9.63197649e-02, 0.00000000e+00, 0.00000000e+00],\n",
            "       [0.00000000e+00, 2.53594786e-01, 0.00000000e+00, 5.20896733e-01,\n",
            "        0.00000000e+00, 5.35555243e-01, 8.14746499e-01, 0.00000000e+00,\n",
            "        6.92805052e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
            "        0.00000000e+00, 0.00000000e+00, 2.63724059e-01, 0.00000000e+00,\n",
            "        6.91636652e-02, 4.14737761e-02, 0.00000000e+00, 1.04966557e+00],\n",
            "       [6.46426439e-01, 1.50868654e+00, 8.63610983e-01, 0.00000000e+00,\n",
            "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
            "        0.00000000e+00, 0.00000000e+00, 1.71027541e+00, 7.04282641e-01,\n",
            "        1.37775615e-01, 2.45563507e-01, 0.00000000e+00, 1.63247609e+00,\n",
            "        0.00000000e+00, 3.32190901e-01, 0.00000000e+00, 0.00000000e+00],\n",
            "       [0.00000000e+00, 4.74895537e-01, 2.21666828e-01, 0.00000000e+00,\n",
            "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
            "        0.00000000e+00, 3.03168654e-01, 0.00000000e+00, 0.00000000e+00,\n",
            "        0.00000000e+00, 3.79627109e-01, 0.00000000e+00, 1.18947220e+00,\n",
            "        0.00000000e+00, 9.71622109e-01, 5.93435287e-01, 0.00000000e+00],\n",
            "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
            "        1.80893257e-01, 8.57927084e-01, 1.80431847e-02, 0.00000000e+00,\n",
            "        0.00000000e+00, 1.28371465e+00, 0.00000000e+00, 0.00000000e+00,\n",
            "        0.00000000e+00, 7.52965927e-01, 0.00000000e+00, 2.16387168e-01,\n",
            "        0.00000000e+00, 9.41006482e-01, 9.07628000e-01, 0.00000000e+00],\n",
            "       [0.00000000e+00, 1.27070582e+00, 9.32552218e-01, 0.00000000e+00,\n",
            "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
            "        0.00000000e+00, 0.00000000e+00, 7.53411829e-01, 1.00481713e+00,\n",
            "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.99604583e-01,\n",
            "        8.41900855e-02, 4.31666762e-01, 5.88299707e-02, 3.44107896e-01],\n",
            "       [9.56669688e-01, 7.71587312e-01, 1.09119236e+00, 0.00000000e+00,\n",
            "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.01584502e-01,\n",
            "        0.00000000e+00, 0.00000000e+00, 1.94212389e+00, 5.70755124e-01,\n",
            "        3.45993221e-01, 3.25538635e-01, 0.00000000e+00, 1.39824069e+00,\n",
            "        1.20899275e-01, 3.10737133e-01, 0.00000000e+00, 0.00000000e+00],\n",
            "       [5.29770613e-01, 1.53905487e+00, 0.00000000e+00, 0.00000000e+00,\n",
            "        8.70227695e-01, 0.00000000e+00, 1.52136385e+00, 0.00000000e+00,\n",
            "        0.00000000e+00, 0.00000000e+00, 3.42397547e+00, 9.16627944e-01,\n",
            "        1.19480121e+00, 5.67052782e-01, 0.00000000e+00, 5.95940828e-01,\n",
            "        0.00000000e+00, 1.20182681e+00, 9.35705781e-01, 0.00000000e+00],\n",
            "       [0.00000000e+00, 3.57925057e-01, 0.00000000e+00, 1.26101327e+00,\n",
            "        0.00000000e+00, 3.86099905e-01, 8.54957223e-01, 0.00000000e+00,\n",
            "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
            "        0.00000000e+00, 1.18523903e-01, 0.00000000e+00, 0.00000000e+00,\n",
            "        0.00000000e+00, 7.77080119e-01, 4.46949869e-01, 8.59493792e-01],\n",
            "       [9.63983893e-01, 3.35652620e-01, 9.58877742e-01, 0.00000000e+00,\n",
            "        1.73325613e-01, 0.00000000e+00, 0.00000000e+00, 3.83377701e-01,\n",
            "        0.00000000e+00, 0.00000000e+00, 1.30418324e+00, 2.00178847e-01,\n",
            "        2.21256346e-01, 3.17745298e-01, 0.00000000e+00, 1.24841881e+00,\n",
            "        4.29850072e-02, 1.27294987e-01, 0.00000000e+00, 0.00000000e+00],\n",
            "       [0.00000000e+00, 0.00000000e+00, 9.48281884e-02, 0.00000000e+00,\n",
            "        0.00000000e+00, 1.69047549e-01, 0.00000000e+00, 0.00000000e+00,\n",
            "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.55871832e-02,\n",
            "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.55953571e-01,\n",
            "        4.26098891e-02, 3.79811615e-01, 1.67231518e-03, 1.89731225e-01],\n",
            "       [9.79914069e-01, 1.56483197e+00, 0.00000000e+00, 0.00000000e+00,\n",
            "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
            "        0.00000000e+00, 1.51396954e+00, 4.87736988e+00, 0.00000000e+00,\n",
            "        2.99899387e+00, 1.38622856e+00, 0.00000000e+00, 1.72414935e+00,\n",
            "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "       [2.83851147e+00, 9.03786361e-01, 2.52315950e+00, 0.00000000e+00,\n",
            "        4.99635309e-01, 0.00000000e+00, 0.00000000e+00, 1.43691748e-01,\n",
            "        2.06049800e-01, 7.59877916e-03, 1.79525614e-01, 0.00000000e+00,\n",
            "        8.66245687e-01, 1.82821286e+00, 0.00000000e+00, 2.10543990e-01,\n",
            "        1.50949836e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "       [0.00000000e+00, 1.14111966e-02, 0.00000000e+00, 1.68234575e+00,\n",
            "        2.44661403e+00, 1.95380068e+00, 5.65710902e-01, 9.18515146e-01,\n",
            "        0.00000000e+00, 1.12154925e+00, 0.00000000e+00, 6.87088609e-01,\n",
            "        1.16486716e+00, 0.00000000e+00, 1.22213793e+00, 0.00000000e+00,\n",
            "        0.00000000e+00, 0.00000000e+00, 1.69305694e+00, 2.05419332e-01],\n",
            "       [0.00000000e+00, 2.25708187e-01, 0.00000000e+00, 1.78837526e+00,\n",
            "        2.05793047e+00, 1.74677563e+00, 2.51245126e-02, 7.59011865e-01,\n",
            "        0.00000000e+00, 1.13996899e+00, 0.00000000e+00, 9.48290825e-01,\n",
            "        9.53315139e-01, 0.00000000e+00, 5.47415614e-01, 0.00000000e+00,\n",
            "        0.00000000e+00, 0.00000000e+00, 2.00272584e+00, 1.72090188e-01],\n",
            "       [1.83942229e-01, 0.00000000e+00, 7.88219035e-01, 0.00000000e+00,\n",
            "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.85984510e-01,\n",
            "        7.63355017e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
            "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
            "        5.09961426e-01, 0.00000000e+00, 0.00000000e+00, 3.91409636e-01],\n",
            "       [1.45098996e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
            "        3.60985965e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
            "        0.00000000e+00, 9.25892413e-01, 3.62707520e+00, 0.00000000e+00,\n",
            "        1.86967599e+00, 7.91822374e-01, 0.00000000e+00, 9.61110353e-01,\n",
            "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "       [4.85806257e-01, 0.00000000e+00, 0.00000000e+00, 7.31184840e-01,\n",
            "        1.26256108e+00, 1.59308004e+00, 0.00000000e+00, 9.34718251e-01,\n",
            "        2.51443505e-01, 1.06241059e+00, 0.00000000e+00, 4.53567475e-01,\n",
            "        9.77442801e-01, 0.00000000e+00, 9.95416567e-02, 0.00000000e+00,\n",
            "        0.00000000e+00, 0.00000000e+00, 8.34222674e-01, 2.77468562e-01],\n",
            "       [7.17143536e-01, 0.00000000e+00, 0.00000000e+00, 1.03617430e+00,\n",
            "        2.23421431e+00, 1.63735271e+00, 0.00000000e+00, 1.55319595e+00,\n",
            "        0.00000000e+00, 1.42719829e+00, 1.90116033e-01, 1.19604647e+00,\n",
            "        1.87593246e+00, 0.00000000e+00, 6.47810698e-01, 0.00000000e+00,\n",
            "        3.19418252e-01, 0.00000000e+00, 1.61257684e+00, 3.23266573e-02],\n",
            "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.42893732e+00,\n",
            "        0.00000000e+00, 1.18275213e+00, 8.95123556e-02, 1.78136423e-01,\n",
            "        5.60196266e-02, 6.47545457e-01, 0.00000000e+00, 0.00000000e+00,\n",
            "        0.00000000e+00, 2.21831739e-01, 2.92280525e-01, 0.00000000e+00,\n",
            "        0.00000000e+00, 2.29940072e-01, 5.77187240e-01, 5.56582928e-01],\n",
            "       [0.00000000e+00, 1.37539938e-01, 0.00000000e+00, 3.37118596e-01,\n",
            "        0.00000000e+00, 7.20616877e-01, 7.70258129e-01, 0.00000000e+00,\n",
            "        1.28468955e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
            "        0.00000000e+00, 0.00000000e+00, 3.69321704e-01, 0.00000000e+00,\n",
            "        4.08008210e-02, 0.00000000e+00, 0.00000000e+00, 1.41330683e+00],\n",
            "       [1.09935224e+00, 6.26486957e-01, 1.28858936e+00, 0.00000000e+00,\n",
            "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.81133762e-01,\n",
            "        0.00000000e+00, 0.00000000e+00, 2.18104792e+00, 6.55728757e-01,\n",
            "        3.35194290e-01, 2.51543969e-01, 0.00000000e+00, 1.43263960e+00,\n",
            "        3.48615617e-01, 3.91965568e-01, 0.00000000e+00, 0.00000000e+00],\n",
            "       [0.00000000e+00, 8.83621201e-02, 0.00000000e+00, 1.09908843e+00,\n",
            "        4.85113174e-01, 9.39675033e-01, 7.45504975e-01, 0.00000000e+00,\n",
            "        0.00000000e+00, 1.88658193e-01, 0.00000000e+00, 0.00000000e+00,\n",
            "        0.00000000e+00, 1.09797493e-01, 6.78445637e-01, 2.92105041e-02,\n",
            "        0.00000000e+00, 4.28513020e-01, 7.72093892e-01, 6.91653490e-01],\n",
            "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.61936212e-01,\n",
            "        0.00000000e+00, 1.06814635e+00, 1.21570297e-01, 1.93338305e-01,\n",
            "        8.74813259e-01, 2.52986073e-01, 0.00000000e+00, 0.00000000e+00,\n",
            "        0.00000000e+00, 0.00000000e+00, 4.53517258e-01, 0.00000000e+00,\n",
            "        5.88721693e-01, 8.09307620e-02, 0.00000000e+00, 7.62032747e-01],\n",
            "       [0.00000000e+00, 4.53563124e-01, 3.37035835e-01, 1.57390274e-02,\n",
            "        0.00000000e+00, 1.64839685e-01, 4.66369599e-01, 0.00000000e+00,\n",
            "        6.71750724e-01, 0.00000000e+00, 0.00000000e+00, 6.86794594e-02,\n",
            "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
            "        3.48060310e-01, 0.00000000e+00, 0.00000000e+00, 1.16139293e+00],\n",
            "       [0.00000000e+00, 3.15771818e-01, 1.42371252e-01, 4.83938396e-01,\n",
            "        0.00000000e+00, 0.00000000e+00, 3.03053409e-01, 0.00000000e+00,\n",
            "        2.38835096e-01, 0.00000000e+00, 0.00000000e+00, 1.23869805e-02,\n",
            "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.46017477e-01,\n",
            "        0.00000000e+00, 2.92995006e-01, 0.00000000e+00, 3.54725361e-01],\n",
            "       [0.00000000e+00, 7.64003471e-02, 0.00000000e+00, 1.04053247e+00,\n",
            "        1.40400553e+00, 1.41013837e+00, 5.16451120e-01, 5.25877476e-01,\n",
            "        0.00000000e+00, 5.18363953e-01, 0.00000000e+00, 5.08523881e-01,\n",
            "        8.40822220e-01, 0.00000000e+00, 8.04635406e-01, 0.00000000e+00,\n",
            "        2.50469178e-01, 0.00000000e+00, 1.10952747e+00, 6.31318748e-01],\n",
            "       [9.17964816e-01, 6.35892868e-01, 1.02667129e+00, 0.00000000e+00,\n",
            "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.93940008e-01,\n",
            "        0.00000000e+00, 0.00000000e+00, 1.52135968e+00, 4.53488886e-01,\n",
            "        2.40330428e-01, 2.44527027e-01, 0.00000000e+00, 1.39044893e+00,\n",
            "        2.51918912e-01, 2.02154726e-01, 0.00000000e+00, 0.00000000e+00]],\n",
            "      dtype=float32)>)\n",
            "Training and Executing Boston Fault Injection:True ABFT: True\n",
            "Epoch 1/5\n",
            "13/13 [==============================] - 0s 28ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
            "Epoch 2/5\n",
            "13/13 [==============================] - 0s 28ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
            "Epoch 3/5\n",
            "13/13 [==============================] - 0s 28ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
            "Epoch 4/5\n",
            "13/13 [==============================] - 0s 30ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
            "Epoch 5/5\n",
            "13/13 [==============================] - 0s 28ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan\n",
            "RMSE Error: nan\n",
            "training_time: 2.8748717308044434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_time=[]\n",
        "sdc_percentage=[]\n",
        "\n",
        "training_epochs = 50\n",
        "train,sdc_1 = do_boston(False, False, training_epochs, False, False)\n",
        "\n",
        "training_time.append(train)\n",
        "sdc_percentage.append(sdc_1)\n",
        "\n",
        "train,sdc_abft = do_boston(False, True, training_epochs, False, False)\n",
        "training_time.append(train)\n",
        "sdc_percentage.append(sdc_abft)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eExSUp6864U5",
        "outputId": "8e06ca6a-4640-43c9-844e-ada39eb4effa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and Executing Boston Fault Injection:False ABFT: False\n",
            "RMSE Error: 8.712405723379137\n",
            "training_time: 11.8464515209198\n",
            "Training and Executing Boston Fault Injection:False ABFT: True\n",
            "RMSE Error: 8.712405723379137\n",
            "training_time: 12.733981847763062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# fig, ax = plt.subplots()\n",
        "tensor_models = ['Regular','ABFT Detection Enable']\n",
        "# ax.bar(tensor_models,[sdc_percentage[0],sdc_percentage[1]])\n",
        "# ax.set_title('RMSE rate on Boston Housing model')\n",
        "# ax.set_xticks(x, models)\n",
        "\n",
        "# plt.show()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(tensor_models,[training_time[0],training_time[1]])\n",
        "ax.set_ylabel('Training time (s)')\n",
        "ax.set_title(f\"Training time Boston Housing model with {training_epochs} epochs (Same RMSE)\")\n",
        "# ax.set_xticks(x, models)\n",
        "\n",
        "plt.show()\n",
        "print(f\"Overhead: {(1.0 - training_time[0]/training_time[1])}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "GFSukQxqCbRZ",
        "outputId": "28f49115-be1a-471e-955f-19356918429b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overhead: 0.03421800676641551%\n"
          ]
        }
      ]
    }
  ]
}